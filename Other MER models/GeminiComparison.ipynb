{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVnfLq2wMDS0",
        "outputId": "bcec4858-895c-4f9c-bcdc-8823963438ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAfcR-JpLVy5"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai nltk\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECGGKwESLtsL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziXrMU3fLvWi"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key=\"your_api_key\")\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owy5qRygL8tJ"
      },
      "outputs": [],
      "source": [
        "def tokenize_latex(expr):\n",
        "    return re.findall(r'(\\\\[a-zA-Z]+|[{}_^=+\\-*/(),]|[a-zA-Z]+|\\d+)', expr)\n",
        "\n",
        "def compute_metrics(preds, gts):\n",
        "    assert len(preds) == len(gts)\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    edit_dists, bleus, codebleus = [], [], []\n",
        "\n",
        "    for pred, gt in zip(preds, gts):\n",
        "        bleu = sentence_bleu([tokenize_latex(gt)], tokenize_latex(pred), smoothing_function=smoothie)\n",
        "        codebleu = sentence_bleu([list(gt)], list(pred), smoothing_function=smoothie)\n",
        "        bleus.append(bleu)\n",
        "        codebleus.append(codebleu)\n",
        "\n",
        "    return {\n",
        "        \"bleu\": sum(bleus) / len(bleus),\n",
        "        \"codebleu\": sum(codebleus) / len(codebleus)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3YWCuyWzS_O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import jiwer\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "image_folder = \"/content/drive/MyDrive/Printed_Split_set/test/Images\"\n",
        "gt_txt_file = \"/content/drive/MyDrive/Printed_Split_set/test/Latex.txt\"\n",
        "output_txt_file = \"./LATEST_PRINTED_gemini_output.txt\"\n",
        "max_samples = None  # Set to integer if needed\n",
        "\n",
        "# === LaTeX Tokenizer\n",
        "def tokenize_latex(expr):\n",
        "    return re.findall(r'(\\\\[a-zA-Z]+|[{}_^=+\\-*/(),]|[a-zA-Z]+|\\d+)', expr)\n",
        "\n",
        "# === CER Tokenizer Transform\n",
        "class TokenizeTransform(jiwer.transforms.AbstractTransform):\n",
        "    def process_string(self, s: str):\n",
        "        return tokenize_latex(s)\n",
        "    def process_list(self, tokens: list[str]):\n",
        "        return [self.process_string(token) for token in tokens]\n",
        "\n",
        "def compute_cer(truth_and_output: list[tuple[str, str]]) -> float:\n",
        "    ground_truth = []\n",
        "    model_output = []\n",
        "\n",
        "    for i, (gt, pred) in enumerate(truth_and_output):\n",
        "        try:\n",
        "            # Coerce to string if not already\n",
        "            gt = str(gt).strip().replace(\"\\n\", \" \") if gt else \"\"\n",
        "            pred = str(pred).strip().replace(\"\\n\", \" \") if pred else \"\"\n",
        "\n",
        "            if gt == \"\" and pred == \"\":\n",
        "                continue  # skip blank pairs\n",
        "\n",
        "            ground_truth.append(gt)\n",
        "            model_output.append(pred)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[CER Skipped] Sample {i} due to error: {e}\")\n",
        "\n",
        "    if not ground_truth or not model_output:\n",
        "        print(\"[CER Warning] No valid pairs found. Returning CER = 1.0\")\n",
        "        return 1.0\n",
        "\n",
        "    try:\n",
        "        return jiwer.cer(\n",
        "            truth=ground_truth,\n",
        "            hypothesis=model_output,\n",
        "            reference_transform=TokenizeTransform(),\n",
        "            hypothesis_transform=TokenizeTransform()\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"[CER Failure] jiwer.cer failed with: {e}\")\n",
        "        return 1.0\n",
        "\n",
        "\n",
        "# === Metric Computation\n",
        "def compute_metrics(preds: list[str], gts: list[str]):\n",
        "    assert len(preds) == len(gts)\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleus = []\n",
        "    truth_and_preds = []\n",
        "\n",
        "    for pred, gt in zip(preds, gts):\n",
        "        if not isinstance(pred, str):\n",
        "            pred = str(pred) if pred is not None else \"\"\n",
        "        if not isinstance(gt, str):\n",
        "            gt = str(gt) if gt is not None else \"\"\n",
        "\n",
        "        pred = pred.strip().replace(\"\\n\", \" \")\n",
        "        gt = gt.strip().replace(\"\\n\", \" \")\n",
        "\n",
        "        bleu = sentence_bleu([tokenize_latex(gt)], tokenize_latex(pred), smoothing_function=smoothie)\n",
        "\n",
        "        bleus.append(bleu)\n",
        "        truth_and_preds.append((gt, pred))\n",
        "\n",
        "    cer = compute_cer(truth_and_preds)\n",
        "\n",
        "    return {\n",
        "        \"bleu\": sum(bleus) / len(bleus),\n",
        "        \"cer\": cer\n",
        "    }\n",
        "\n",
        "\n",
        "# === Retry wrapper for Gemini inference\n",
        "def get_latex_from_gemini(prompt, image, max_retries=3, retry_delay=2):\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            response = model.generate_content([prompt, image])\n",
        "            return response.text.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"[Attempt {attempt}] Error: {e}\")\n",
        "            if attempt < max_retries:\n",
        "                print(f\"Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(\"Max retries reached. Skipping.\")\n",
        "                return \"\"\n",
        "\n",
        "# === Load files\n",
        "image_files = sorted([\n",
        "    f for f in os.listdir(image_folder)\n",
        "    if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "], key=lambda x: int(os.path.splitext(x)[0]))\n",
        "\n",
        "with open(gt_txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    gt_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "assert len(image_files) == len(gt_lines), \"Mismatch between image and label count\"\n",
        "\n",
        "if max_samples:\n",
        "    image_files = image_files[:max_samples]\n",
        "    gt_lines = gt_lines[:max_samples]\n",
        "\n",
        "# === Inference + evaluation\n",
        "predictions = []\n",
        "bleu_scores = []\n",
        "truth_and_preds = []\n",
        "\n",
        "with open(output_txt_file, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for i, fname in enumerate(tqdm(image_files)):\n",
        "        try:\n",
        "            img_path = os.path.join(image_folder, fname)\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            prompt = \"Extract the optimization problem in this image and return only the LaTeX code. I want clean latex in a single line without any begin and end tags, without dollar sign. Example: \\\\text{max} \\\\quad & 6x_1 + \\\\log(x_2 + 1) \\\\\\\\ \\\\text{st} \\\\quad & e^{x_1} + 2x_2 + w_1 = 17 \\\\\\\\ & x_1, x_2, w_1, w_2, w_3 \\\\geq 0.\"\n",
        "\n",
        "            pred = get_latex_from_gemini(prompt, image)\n",
        "            gt = gt_lines[i]\n",
        "\n",
        "            predictions.append(pred)\n",
        "            truth_and_preds.append((gt, pred))\n",
        "\n",
        "            bleu = sentence_bleu([tokenize_latex(gt)], tokenize_latex(pred), smoothing_function=SmoothingFunction().method4)\n",
        "\n",
        "\n",
        "            bleu_scores.append(bleu)\n",
        "\n",
        "            fout.write(f\"image: {fname}\\n\")\n",
        "            fout.write(f\"gt    : {gt}\\n\")\n",
        "            fout.write(f\"prediction: {pred}\\n\")\n",
        "            fout.write(f\"bleu  : {bleu:.4f}\")\n",
        "            fout.write(\"-\" * 60 + \"\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected failure on {fname}: {e}\")\n",
        "            predictions.append(\"\")\n",
        "            truth_and_preds.append((gt_lines[i], \"\"))\n",
        "\n",
        "if len(predictions) != len(gt_lines):\n",
        "    raise ValueError(f\"Length mismatch! Predictions: {len(predictions)} vs GT: {len(gt_lines)}\")\n",
        "\n",
        "# === Final Evaluation Summary\n",
        "metrics = compute_metrics(predictions, gt_lines)\n",
        "\n",
        "with open(output_txt_file, \"a\", encoding=\"utf-8\") as fout:\n",
        "    fout.write(\"\\n=== Summary ===\\n\")\n",
        "    fout.write(f\"Total Samples    : {len(predictions)}\\n\")\n",
        "    fout.write(f\"Average BLEU     : {metrics['bleu']:.4f}\\n\")\n",
        "    fout.write(f\"Average CER      : {metrics['cer']:.4f}\\n\")\n",
        "\n",
        "print(\"\\n=== Evaluation Complete ===\")\n",
        "print(f\"Samples evaluated : {len(predictions)}\")\n",
        "print(f\"Average BLEU      : {metrics['bleu']:.4f}\")\n",
        "print(f\"Average CER       : {metrics['cer']:.4f}\")\n",
        "print(f\"Output saved to   : {output_txt_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTkmzAmDe8hO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
